{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3c9b5bf-3a05-4cce-864c-692c760c0f86",
   "metadata": {},
   "source": [
    "# SparkSession\n",
    "Since Spark 2.0 SparkSession has become an entry point to PySpark to work with RDD, and DataFrame. Prior to 2.0, SparkContext used to be an entry point. Here, I will mainly focus on explaining what is SparkSession by defining and describing how to create SparkSession and using default SparkSession spark variable from pyspark-shell.\n",
    "\n",
    "#### What is SparkSession?\n",
    "SparkSession was introduced in version 2.0, It is an entry point to underlying PySpark functionality in order to programmatically create PySpark RDD, DataFrame. It’s object spark is default available in pyspark-shell and it can be created programmatically using SparkSession.\n",
    "\n",
    "With Spark 2.0 a new class SparkSession (pyspark.sql import SparkSession) has been introduced. SparkSession is a combined class for all different contexts we used to have prior to 2.0 release (SQLContext and HiveContext e.t.c). Since 2.0 SparkSession can be used in replace with SQLContext, HiveContext, and other contexts defined prior to 2.0.\n",
    "\n",
    "As mentioned in the beginning SparkSession is an entry point to PySpark and creating a SparkSession instance would be the first statement you would write to program with RDD, DataFrame, and Dataset. SparkSession will be created using SparkSession.builder builder patterns.\n",
    "\n",
    "Though SparkContext used to be an entry point prior to 2.0, It is not completely replaced with SparkSession, many features of SparkContext are still available and used in Spark 2.0 and later. You should also know that SparkSession internally creates SparkConfig and SparkContext with the configuration provided with SparkSession.\n",
    "\n",
    "SparkSession also includes all the APIs available in different contexts –\n",
    "* SparkContext,\n",
    "* SQLContext,\n",
    "* StreamingContext,\n",
    "* HiveContext.\n",
    "\n",
    "#### How many SparkSessions can you create in a PySpark application?\n",
    "You can create as many SparkSession as you want in a PySpark application using either SparkSession.builder() or SparkSession.newSession(). Many Spark session objects are required when you wanted to keep PySpark tables (relational entities) logically separated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9f1379-55da-4303-a747-192cf8f448a1",
   "metadata": {},
   "source": [
    "## SparkSession in PySpark shell\n",
    "Be default PySpark shell provides “spark” object; which is an instance of SparkSession class. We can directly use this object where required in spark-shell. Start your “pyspark” shell from $SPARK_HOME\\bin folder and enter the pyspark command.\n",
    "\n",
    "Once you are in the PySpark shell enter the below command to get the PySpark version.\n",
    "```\n",
    "# Usage of spark object in PySpark shell\n",
    ">>>spark.version\n",
    "3.1.2\n",
    "```\n",
    "\n",
    "Similar to the PySpark shell, in most of the tools, the environment itself creates a default SparkSession object for us to use so you don’t have to worry about creating a SparkSession object."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5b4112-77af-4182-867f-d025c31bcd4d",
   "metadata": {},
   "source": [
    "## Create SparkSession\n",
    "In order to create SparkSession programmatically (in .py file) in PySpark, you need to use the builder pattern method builder() as explained below. getOrCreate() method returns an already existing SparkSession; if not exists, it creates a new SparkSession."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ee6565a-3c98-47d7-94ed-e2b1f0769679",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e963f7b2-fcea-468e-9c62-d2ff97d51083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SparkSession from builder\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[1]\") \\\n",
    "                    .appName('SparkByExamples.com') \\\n",
    "                    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02438a2-6786-4070-bebf-0e628e13ec87",
   "metadata": {},
   "source": [
    "master() – If you are running it on the cluster you need to use your master name as an argument to master(). usually, it would be either yarn or mesos depends on your cluster setup.\n",
    "\n",
    "Use local[x] when running in Standalone mode. x should be an integer value and should be greater than 0; this represents how many partitions it should create when using RDD, DataFrame, and Dataset. Ideally, x value should be the number of CPU cores you have.\n",
    "appName() – Used to set your application name.\n",
    "\n",
    "getOrCreate() – This returns a SparkSession object if already exists, and creates a new one if not exist.\n",
    "\n",
    "Note:  SparkSession object spark is by default available in the PySpark shell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6d618d-30de-4c05-866e-affff8b819e7",
   "metadata": {},
   "source": [
    "### Create Another SparkSession\n",
    "You can also create a new SparkSession using newSession() method. This uses the same app name, master as the existing session. Underlying SparkContext will be the same for both sessions as you can have only one context per PySpark application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed0550c9-eb72-408d-a232-d7acce5b8501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x0000028838B61F88>\n"
     ]
    }
   ],
   "source": [
    "# Create new SparkSession\n",
    "spark2 = spark.newSession()\n",
    "print(spark2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7cd5d6-5082-4b1a-80c4-02aa4c409106",
   "metadata": {},
   "source": [
    "This always creates a new SparkSession object."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4d240b-4ef0-4a1b-9991-1c05714700df",
   "metadata": {},
   "source": [
    "### Get Existing SparkSession\n",
    "You can get the existing SparkSession in PySpark using the builder.getOrCreate(), for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93eb165b-6a05-4425-9e75-7ee2d9190f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x0000028838B59688>\n"
     ]
    }
   ],
   "source": [
    "# Get Existing SparkSession\n",
    "spark3 = SparkSession.builder.getOrCreate()\n",
    "print(spark3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3852bbc7-a6a1-44e2-9234-350d603f6596",
   "metadata": {},
   "source": [
    "### Using Spark Config\n",
    "If you wanted to set some configs to SparkSession, use the config() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f3e5679-c39a-4818-8a42-2e74589b32ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage of config()\n",
    "spark = SparkSession.builder \\\n",
    "      .master(\"local[1]\") \\\n",
    "      .appName(\"SparkByExamples.com\") \\\n",
    "      .config(\"spark.some.config.option\", \"config-value\") \\\n",
    "      .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d1c8c7-19e1-40c9-97a3-25b8523aa46a",
   "metadata": {},
   "source": [
    "### Create SparkSession with Hive Enable\n",
    "In order to use Hive with PySpark, you need to enable it using the enableHiveSupport() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a0de159-e561-4549-b5bc-d8fcf26afcc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enabling Hive to use in Spark\n",
    "spark = SparkSession.builder \\\n",
    "      .master(\"local[1]\") \\\n",
    "      .appName(\"SparkByExamples.com\") \\\n",
    "      .config(\"spark.sql.warehouse.dir\", \"<path>/spark-warehouse\") \\\n",
    "      .enableHiveSupport() \\\n",
    "      .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11a8cb8-8650-4030-8b03-83f7da1e9d00",
   "metadata": {},
   "source": [
    "### Using PySpark Configs\n",
    "Once the SparkSession is created, you can add the spark configs during runtime or get all configs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ffd56712-f8de-4324-bde6-86649b244d87",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Cannot modify the value of a Spark config: spark.executor.memory;",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14260\\3372151909.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Set Config\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"spark.executor.memory\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"5g\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\spark\\python\\pyspark\\sql\\conf.py\u001b[0m in \u001b[0;36mset\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[1;34m\"\"\"Sets the given Spark runtime configuration property.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jconf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1305\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1307\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    132\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m                 \u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(e)\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: Cannot modify the value of a Spark config: spark.executor.memory;"
     ]
    }
   ],
   "source": [
    "# Set Config\n",
    "spark.conf.set(\"spark.executor.memory\", \"5g\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "269525b2-9ec3-46d4-99ab-9ce2d4d0f5c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "# Get a Spark Config\n",
    "partitions = spark.conf.get(\"spark.sql.shuffle.partitions\")\n",
    "print(partitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042c772b-c640-45ad-b31e-4d5266550bbd",
   "metadata": {},
   "source": [
    "## Create PySpark DataFrame\n",
    "SparkSession also provides several methods to create a Spark DataFrame and DataSet. The below example uses the createDataFrame() method which takes a list of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80f8cbce-cd10-46c5-86a0-157cd96eea92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|   _1|   _2|\n",
      "+-----+-----+\n",
      "|Scala|25000|\n",
      "|Spark|35000|\n",
      "|  PHP|21000|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create DataFrame\n",
    "df = spark.createDataFrame(\n",
    "    [(\"Scala\", 25000), (\"Spark\", 35000), (\"PHP\", 21000)])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c9c782-771a-4eb3-a11e-015d02ff7a14",
   "metadata": {},
   "source": [
    "## Working with Spark SQL\n",
    "Using SparkSession you can access PySpark/Spark SQL capabilities in PySpark. In order to use SQL features first, you need to create a temporary view in PySpark. Once you have a temporary view you can run any ANSI SQL queries using spark.sql() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ee6d33d-687b-4d9c-9af6-87be315e93e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|   _1|   _2|\n",
      "+-----+-----+\n",
      "|Scala|25000|\n",
      "|Spark|35000|\n",
      "|  PHP|21000|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Spark SQL\n",
    "df.createOrReplaceTempView(\"sample_table\")\n",
    "df2 = spark.sql(\"SELECT _1,_2 FROM sample_table\")\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f0a835-5469-4a9e-bf38-c0b7278b36cf",
   "metadata": {},
   "source": [
    "PySpark SQL temporary views are session-scoped and will not be available if the session that creates it terminates. If you want to have a temporary view that is shared among all sessions and keep alive until the Spark application terminates, you can create a global temporary view using createGlobalTempView()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc715980-b64d-47fc-9d18-3d22c55c783b",
   "metadata": {},
   "source": [
    "## Create Hive Table\n",
    "As explained above SparkSession is used to create and query Hive tables. Note that in order to do this for testing you don’t need Hive to be installed. saveAsTable() creates Hive managed table. Query the table using spark.sql()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "90ee4d3b-2e1c-48d4-a143-b3d4f2e2f962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|   _1|   _2|\n",
      "+-----+-----+\n",
      "|Scala|25000|\n",
      "|Spark|35000|\n",
      "|  PHP|21000|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create Hive table & query it.  \n",
    "spark.table(\"sample_table\").write.saveAsTable(\"sample_hive_table\")\n",
    "df3 = spark.sql(\"SELECT _1,_2 FROM sample_hive_table\")\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176b86b7-1bcb-47bf-ad05-f149958009c4",
   "metadata": {},
   "source": [
    "## Working with Catalogs\n",
    "To get the catalog metadata, PySpark Session exposes catalog variable. Note that these methods spark.catalog.listDatabases and spark.catalog.listTables and returns the DataSet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "666bed5c-4cfe-4181-8ac4-3cd3fc0ae7f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Database(name='default', description='default database', locationUri='file:/C:/Users/prajyot.naik/Downloads/pyspark_by_examples/spark/pyspark/notebooks/spark-warehouse')]\n"
     ]
    }
   ],
   "source": [
    "# Get metadata from the Catalog\n",
    "# List databases\n",
    "dbs = spark.catalog.listDatabases()\n",
    "print(dbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97229fbd-9120-48d5-a0ca-1e197bc3a0ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Table(name='sample_hive_table', database='default', description=None, tableType='MANAGED', isTemporary=False), Table(name='sample_table', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]\n"
     ]
    }
   ],
   "source": [
    "# List Tables\n",
    "tbls = spark.catalog.listTables()\n",
    "print(tbls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5f8f38-743c-4c69-b35a-b2f663797093",
   "metadata": {},
   "source": [
    "## SparkSession Commonly Used Methods\n",
    "* version() – Returns the Spark version where your application is running, probably the Spark version your cluster is configured with.\n",
    "* createDataFrame() – This creates a DataFrame from a collection and an RDD\n",
    "* getActiveSession() – returns an active Spark session.\n",
    "* read() – Returns an instance of DataFrameReader class, this is used to read records from csv, parquet, avro, and more file formats into DataFrame.\n",
    "* readStream() – Returns an instance of DataStreamReader class, this is used to read streaming data. that can be used to read streaming data into DataFrame.\n",
    "* sparkContext() – Returns a SparkContext.\n",
    "* sql() – Returns a DataFrame after executing the SQL mentioned.\n",
    "* sqlContext() – Returns SQLContext.\n",
    "* stop() – Stop the current SparkContext.\n",
    "* table() – Returns a DataFrame of a table or view.\n",
    "* udf() – Creates a PySpark UDF to use it on DataFrame, Dataset, and SQL."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

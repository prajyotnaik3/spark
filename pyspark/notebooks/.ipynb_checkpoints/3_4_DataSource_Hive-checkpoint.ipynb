{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e93e0533-4f91-40b2-9285-1124a1c5679c",
   "metadata": {},
   "source": [
    "# PySpark - Hive\n",
    "## PySpark Save DataFrame to Hive Table\n",
    "To save a PySpark DataFrame to Hive table use saveAsTable() function or use SQL CREATE statement on top of the temporary view. In order to save DataFrame as a Hive table in PySpark, you need to create a SparkSession with enableHiveSupport().\n",
    "\n",
    "This method is available pyspark.sql.SparkSession.builder.enableHiveSupport() which enables Hive support, including connectivity to a persistent Hive metastore, support for Hive SerDes, and Hive user-defined functions.\n",
    "\n",
    "Following are the Steps to Save PySpark DataFrame to Hive Table.<br>\n",
    "Step 1 – Create SparkSession with hive enabled<br>\n",
    "Step 2 – Create PySpark DataFrame<br>\n",
    "Step 3 – Save PySpark DataFrame to Hive table<br>\n",
    "Step 4 – Confirm Hive table is created<br>\n",
    "\n",
    "### Create Spark Session with Hive Enabled\n",
    "In order to read the hive table into pySpark DataFrame first, you need to create a SparkSession with Hive support enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4011a90c-a297-4929-924d-d85cc3303ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e5eb6b0-75a3-4d68-adde-b02712eecaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import abspath\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#enableHiveSupport() -> enables sparkSession to connect with Hive\n",
    "warehouse_location = abspath('spark-warehouse')\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"SparkByExamples.com\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", warehouse_location) \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9c12f5-3be9-4eea-9d96-347343dbadb8",
   "metadata": {},
   "source": [
    "### PySpark Save DataFrame to Hive Table\n",
    "By using saveAsTable() from DataFrameWriter you can save or write a PySpark DataFrame to a Hive table. Pass the table name you wanted to save as an argument to this function and make sure the table name is in the form of database.tablename. If the database doesn’t exist, you will get an error. To start with you can also try just the table name without a database.\n",
    "\n",
    "You can use this to write PySpark DataFrame to a new Hive table or overwrite an existing table. PySpark writes the data to the default Hive warehouse location which is /user/hive/warehouse when you use a Hive cluster. But on local it creates in the current directory. You can change this behavior, using the spark.sql.warehouse.dir configuration while creating a SparkSession .\n",
    "\n",
    "Since we are running it locally from IntelliJ, it creates a metadata database metastore_db and spark-warehouse under the current directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10adfad3-9aa8-430b-b2d4-626c8d22f1f3",
   "metadata": {},
   "source": [
    "### Save DataFrame as Internal Table from PySpark\n",
    "By default saveAsTable() method saves PySpark DataFrame as a managed Hive table. Managed tables are also known as internal tables that are owned and managed by Hive. By default, Hive creates a table as an Internal table and owned the table structure and the files. When you drop an internal table, it drops the data and also drops the metadata of the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e50f9fb8-3d92-49f2-9b23-05c8fdfdf2a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+------+\n",
      "| id|    name|age|gender|\n",
      "+---+--------+---+------+\n",
      "|  4|Jennifer| 20|     F|\n",
      "|  1|   James| 30|     M|\n",
      "|  3|    Jeff| 41|     M|\n",
      "|  2|     Ann| 40|     F|\n",
      "+---+--------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns = [\"id\", \"name\",\"age\",\"gender\"]\n",
    "\n",
    "# Create DataFrame \n",
    "data = [(1, \"James\",30,\"M\"), (2, \"Ann\",40,\"F\"),\n",
    "    (3, \"Jeff\",41,\"M\"),(4, \"Jennifer\",20,\"F\")]\n",
    "sampleDF = spark.sparkContext.parallelize(data).toDF(columns)\n",
    "\n",
    "# Create Hive Internal table\n",
    "sampleDF.write.mode('overwrite') \\\n",
    "         .saveAsTable(\"employee\")\n",
    "\n",
    "# Read Hive table\n",
    "df = spark.read.table(\"employee\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43648f16-eacf-4765-90b0-f0fb20b88207",
   "metadata": {},
   "source": [
    "It creates the Hive metastore metastore_db and Hive warehouse location spark-warehouse in the current directory. The employee table is created inside the warehouse directory.\n",
    "\n",
    "Also, note that by default it creates files in parquet format with snappy compression.\n",
    "\n",
    "If you wanted to create a table within a Database, use the prefix database name. If you don’t have the database, you can create one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5cb40aa-b9a4-4098-a1a0-311ac1eec3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create database \n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS emp\")\n",
    "\n",
    "# Create Hive Internal table\n",
    "sampleDF.write.mode('overwrite') \\\n",
    "    .saveAsTable(\"emp.employee\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8bfcc0-88eb-4e34-aff4-77c5c12b0de6",
   "metadata": {},
   "source": [
    "### Save as External Table\n",
    "To create an external table use the path of your choice using option(). The data in External tables are not owned or managed by Hive. Dropping an external table just drops the metadata but not the actual data. The actual data is still accessible outside of Hive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "056d1aef-f858-4194-b5b5-e5699d8f5504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Hive External table\n",
    "sampleDF.write.mode('overwrite') \\\n",
    "        .option(\"path\", \"../resources/tmp/employee\") \\\n",
    "        .saveAsTable(\"emp.employee\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb51813-2a61-4ddd-b323-28e42710077d",
   "metadata": {},
   "source": [
    "### Using PySpark SQL Temporary View to Save Hive Table\n",
    "Use SparkSession.sql() method and CREATE TABLE statement to create a table in Hive from PySpark temporary view. Above we have created a temporary view “sampleView“. Now we shall create a Database and Table using SQL in Hive Metastore and insert data into the Hive table using the view we created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31493461-16c9-45fe-821b-473f8aac2b74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+------+\n",
      "| id|    name|age|gender|\n",
      "+---+--------+---+------+\n",
      "|  1|   James| 30|     M|\n",
      "|  2|     Ann| 40|     F|\n",
      "|  3|    Jeff| 41|     M|\n",
      "|  4|Jennifer| 20|     F|\n",
      "+---+--------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create temporary view\n",
    "sampleDF.createOrReplaceTempView(\"sampleView\")\n",
    "\n",
    "# Create a Database CT\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS ct\")\n",
    "\n",
    "# Create a Table naming as sampleTable under CT database.\n",
    "spark.sql(\"CREATE TABLE ct.sampleTable (id Int, name String, age Int, gender String)\")\n",
    "\n",
    "# Insert into sampleTable using the sampleView. \n",
    "spark.sql(\"INSERT INTO TABLE ct.sampleTable  SELECT * FROM sampleView\")\n",
    "\n",
    "# Lets view the data in the table\n",
    "spark.sql(\"SELECT * FROM ct.sampleTable\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79e2783-191b-4077-9bb4-2cf05246d582",
   "metadata": {},
   "source": [
    "\n",
    "## PySpark SQL Read Hive Table\n",
    "PySpark SQL supports reading a Hive table to DataFrame in two ways: the SparkSesseion.read.table() method and the SparkSession.sql() statement.\n",
    "\n",
    "In order to read a Hive table, you need to create a SparkSession with enableHiveSupport(). This method is available at pyspark.sql.SparkSession.builder.enableHiveSupport() which is used to enable Hive support, including connectivity to a persistent Hive metastore, support for Hive SerDes, and Hive user-defined functions.\n",
    "\n",
    "Steps to Read Hive Table into PySpark DataFrame<br>\n",
    "Step 1 – Import PySpark<br>\n",
    "Step 2 – Create SparkSession with Hive enabled<br>\n",
    "Step 3 – Read Hive table into Spark DataFrame using spark.sql()<br>\n",
    "Step 4 – Read using spark.read.table()<br>\n",
    "Step 5 – Connect to remove Hive.<br>\n",
    "\n",
    "PySpark reads the data from the default Hive warehouse location which is /user/hive/warehouse when you use a Hive cluster. But on local, it reads from the current directory. You can change this behavior, using the spark.sql.warehouse.dir configuration while creating a SparkSession .\n",
    "\n",
    "### PySpark Read Hive Table into DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d7c9e41-fe4f-449b-9c97-b072ccd9403f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+------+\n",
      "| id|    name|age|gender|\n",
      "+---+--------+---+------+\n",
      "|  4|Jennifer| 20|     F|\n",
      "|  1|   James| 30|     M|\n",
      "|  3|    Jeff| 41|     M|\n",
      "|  2|     Ann| 40|     F|\n",
      "+---+--------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read Hive table\n",
    "df = spark.sql(\"select * from emp.employee\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5acf1c9-0dbb-436e-b57e-fe2266a55efe",
   "metadata": {},
   "source": [
    "### Using spark.read.table()\n",
    "Alternatively, you can also read by using spark.read.table() method. here, spark.read is an object of the class DataFrameReader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05bc83c3-293d-46cb-b58b-1b0f32d136f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+------+\n",
      "| id|    name|age|gender|\n",
      "+---+--------+---+------+\n",
      "|  4|Jennifer| 20|     F|\n",
      "|  1|   James| 30|     M|\n",
      "|  3|    Jeff| 41|     M|\n",
      "|  2|     Ann| 40|     F|\n",
      "+---+--------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read Hive table\n",
    "df = spark.read.table(\"employee\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d426efa1-dbb6-46d3-848f-2f8a1fba3764",
   "metadata": {},
   "source": [
    "### PySpark Read Hive Table from Remote Hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "69d28bf0-8e65-481b-b8a4-ea542840a8ae",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SparkContext' object has no attribute 'conf'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10876\\2913096675.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m# or Use the below approach\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;31m# Change using conf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"spark.sql.warehouse.dir\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"/user/hive/warehouse\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"hive.metastore.uris\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"thrift://localhost:9083\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'SparkContext' object has no attribute 'conf'"
     ]
    }
   ],
   "source": [
    "from os.path import abspath\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#enableHiveSupport() -> enables sparkSession to connect with Hive\n",
    "warehouse_location = abspath('spark-warehouse')\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"SparkByExamples.com\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"/hive/warehouse/dir\") \\\n",
    "    .config(\"hive.metastore.uris\", \"thrift://remote-host:9083\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# or Use the below approach\n",
    "# Change using conf\n",
    "spark.sparkContext.conf.set(\"spark.sql.warehouse.dir\", \"/user/hive/warehouse\");\n",
    "spark.sparkContext.conf.set(\"hive.metastore.uris\", \"thrift://localhost:9083\");"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

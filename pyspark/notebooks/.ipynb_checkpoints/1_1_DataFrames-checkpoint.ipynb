{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2591ede7-dc16-41fe-9d44-2cb19483620c",
   "metadata": {},
   "source": [
    "# DataFrames\n",
    "In order to run PySpark in Jupyter notebook, first you need to find the PySpark Install. We will use findspark package to do so. Since this is a third-party package we need to install it before using it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db5da1ef-5bb0-4240-8182-1aa0a851956c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"SparkByExamples.com\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9712e83-39c7-4a55-8f54-fe46c7bc78ad",
   "metadata": {},
   "source": [
    "## Create an Empty DataFrame\n",
    "### Create Empty RDD in PySpark\n",
    "Create an empty RDD by using emptyRDD() of SparkContext for example spark.sparkContext.emptyRDD().<br>\n",
    "Alternatively you can also get empty RDD by using spark.sparkContext.parallelize([]).<br>\n",
    "Note: If you try to perform operations on empty RDD you going to get ValueError(\"RDD is empty\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7ea2e61-9bb2-4d62-bdac-8a4c435ec227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EmptyRDD[0] at emptyRDD at NativeMethodAccessorImpl.java:0\n"
     ]
    }
   ],
   "source": [
    "#Creates Empty RDD\n",
    "emptyRDD = spark.sparkContext.emptyRDD()\n",
    "print(emptyRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "960fb0bb-f1af-4b8d-97a9-e5a94c1ef96a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParallelCollectionRDD[1] at readRDDFromFile at PythonRDD.scala:262\n"
     ]
    }
   ],
   "source": [
    "#Creates Empty RDD using parallelize\n",
    "rdd2= spark.sparkContext.parallelize([])\n",
    "print(rdd2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb5c48d-a835-4956-994e-4059da85c725",
   "metadata": {},
   "source": [
    "### Create Empty DataFrame with Schema (StructType)\n",
    "In order to create an empty PySpark DataFrame manually with schema ( column names & data types) first, Create a schema using StructType and StructField."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9176d9a6-405b-48fa-8f63-339d7d46b135",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Schema\n",
    "from pyspark.sql.types import StructType,StructField, StringType\n",
    "schema = StructType([\n",
    "  StructField('firstname', StringType(), True),\n",
    "  StructField('middlename', StringType(), True),\n",
    "  StructField('lastname', StringType(), True)\n",
    "  ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d4d5a8-df1e-4131-baa6-406915d198f0",
   "metadata": {},
   "source": [
    "Now use the empty RDD created above and pass it to createDataFrame() of SparkSession along with the schema for column names & data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "053e75f8-3267-4b98-a4ab-ef57682bee90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- middlename: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Create empty DataFrame from empty RDD\n",
    "df = spark.createDataFrame(emptyRDD,schema)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d175c9e-a5da-4106-a9a9-81f367e454f5",
   "metadata": {},
   "source": [
    "### Convert Empty RDD to DataFrame\n",
    "You can also create empty DataFrame by converting empty RDD to DataFrame using toDF()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e1251be-f3cb-4058-89e5-1e0fb7b08a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- middlename: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Convert empty RDD to Dataframe\n",
    "df1 = emptyRDD.toDF(schema)\n",
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb376824-c6d6-4b1d-82e7-5822ccff2885",
   "metadata": {},
   "source": [
    "### Create Empty DataFrame with Schema\n",
    "We can create empty dataframe manually with schema and without RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28bb35b8-be82-4868-a0d0-6cb91f15cc59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- middlename: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Create empty DataFrame directly.\n",
    "df2 = spark.createDataFrame([], schema)\n",
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d17a9d-2b17-4ee1-9f25-c3642f784b6a",
   "metadata": {},
   "source": [
    "### Create Empty DataFrame without Schema (no columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30b41924-28ea-4928-8523-ee2cad3c6408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Create empty DatFrame with no schema (no columns)\n",
    "df3 = spark.createDataFrame([], StructType([]))\n",
    "df3.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3922b5b-6df5-4a16-b5a4-0bef295fca0b",
   "metadata": {},
   "source": [
    "## Convert PySpark RDD to DataFrame\n",
    "In PySpark, toDF() function of the RDD is used to convert RDD to DataFrame. We would need to convert RDD to DataFrame as DataFrame provides more advantages over RDD. For instance, DataFrame is a distributed collection of data organized into named columns similar to Database tables and provides optimization and performance improvements.\n",
    "### Create PySpark RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16818330-abb0-4664-8bb6-3ad98bedd4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "dept = [(\"Finance\",10),(\"Marketing\",20),(\"Sales\",30),(\"IT\",40)]\n",
    "rdd = spark.sparkContext.parallelize(dept)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f53cd9-537b-4a28-b8c8-707d258d4adc",
   "metadata": {},
   "source": [
    "### Convert PySpark RDD to DataFrame\n",
    "Converting PySpark RDD to DataFrame can be done using toDF(), createDataFrame().\n",
    "#### Using rdd.toDF() function\n",
    "PySpark provides toDF() function in RDD which can be used to convert RDD into Dataframe. By default, toDF() function creates column names as “_1” and “_2”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bba5038a-64ce-4826-91f6-9a7f501e3186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: long (nullable = true)\n",
      "\n",
      "+---------+---+\n",
      "|_1       |_2 |\n",
      "+---------+---+\n",
      "|Finance  |10 |\n",
      "|Marketing|20 |\n",
      "|Sales    |30 |\n",
      "|IT       |40 |\n",
      "+---------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = rdd.toDF()\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3198ce-b10f-45ef-a890-42b7a4ab0cb7",
   "metadata": {},
   "source": [
    "toDF() has another signature that takes arguments to define column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb9bc187-d433-4b16-a04e-966a09bce368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dept_name: string (nullable = true)\n",
      " |-- dept_id: long (nullable = true)\n",
      "\n",
      "+---------+-------+\n",
      "|dept_name|dept_id|\n",
      "+---------+-------+\n",
      "|Finance  |10     |\n",
      "|Marketing|20     |\n",
      "|Sales    |30     |\n",
      "|IT       |40     |\n",
      "+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "deptColumns = [\"dept_name\",\"dept_id\"]\n",
    "df2 = rdd.toDF(deptColumns)\n",
    "df2.printSchema()\n",
    "df2.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbd0610-ff8f-420c-878e-8e22d87cb433",
   "metadata": {},
   "source": [
    "### Using PySpark createDataFrame() function\n",
    "SparkSession class provides createDataFrame() method to create DataFrame and it takes rdd object as an argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "802bc9b4-43fc-4258-bb43-ce220a16fde7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dept_name: string (nullable = true)\n",
      " |-- dept_id: long (nullable = true)\n",
      "\n",
      "+---------+-------+\n",
      "|dept_name|dept_id|\n",
      "+---------+-------+\n",
      "|Finance  |10     |\n",
      "|Marketing|20     |\n",
      "|Sales    |30     |\n",
      "|IT       |40     |\n",
      "+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "deptDF = spark.createDataFrame(rdd, schema = deptColumns)\n",
    "deptDF.printSchema()\n",
    "deptDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a514e8-8fbd-4c91-a8f1-abe20cd88c03",
   "metadata": {},
   "source": [
    "##### Using createDataFrame() with StructType schema\n",
    "When you infer the schema, by default the datatype of the columns is derived from the data and set’s nullable to true for all columns. We can change this behavior by supplying schema using StructType – where we can specify a column name, data type and nullable for each field/column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12449838-2014-47d7-b292-91150ca099b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dept_name: string (nullable = true)\n",
      " |-- dept_id: string (nullable = true)\n",
      "\n",
      "+---------+-------+\n",
      "|dept_name|dept_id|\n",
      "+---------+-------+\n",
      "|Finance  |10     |\n",
      "|Marketing|20     |\n",
      "|Sales    |30     |\n",
      "|IT       |40     |\n",
      "+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "deptSchema = StructType([       \n",
    "    StructField('dept_name', StringType(), True),\n",
    "    StructField('dept_id', StringType(), True)\n",
    "])\n",
    "\n",
    "deptDF1 = spark.createDataFrame(rdd, schema = deptSchema)\n",
    "deptDF1.printSchema()\n",
    "deptDF1.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e374545e-b61a-4a8f-811c-32767027adca",
   "metadata": {},
   "source": [
    "## Convert PySpark DataFrame to Pandas\n",
    "PySpark DataFrame can be converted to Python pandas DataFrame using a function toPandas().<br>\n",
    "Operations on Pyspark run faster than Pandas due to its distributed nature and parallel execution on multiple cores and machines. In other words, pandas run operations on a single node whereas PySpark runs on multiple machines.<br>\n",
    "If you are working on a Machine Learning application where you are dealing with larger datasets, PySpark processes operations many times faster than pandas.<br>\n",
    "After processing data in PySpark we would need to convert it back to Pandas DataFrame for a further procession with Machine Learning application or any Python applications.\n",
    "### Prepare PySpark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a8d09f1-ed8c-4392-a804-d86095798bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- middle_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- dob: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n",
      "+----------+-----------+---------+-----+------+------+\n",
      "|first_name|middle_name|last_name|dob  |gender|salary|\n",
      "+----------+-----------+---------+-----+------+------+\n",
      "|James     |           |Smith    |36636|M     |60000 |\n",
      "|Michael   |Rose       |         |40288|M     |70000 |\n",
      "|Robert    |           |Williams |42114|      |400000|\n",
      "|Maria     |Anne       |Jones    |39192|F     |500000|\n",
      "|Jen       |Mary       |Brown    |     |F     |0     |\n",
      "+----------+-----------+---------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"James\",\"\",\"Smith\",\"36636\",\"M\",60000),\n",
    "        (\"Michael\",\"Rose\",\"\",\"40288\",\"M\",70000),\n",
    "        (\"Robert\",\"\",\"Williams\",\"42114\",\"\",400000),\n",
    "        (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\",500000),\n",
    "        (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",0)]\n",
    "\n",
    "columns = [\"first_name\",\"middle_name\",\"last_name\",\"dob\",\"gender\",\"salary\"]\n",
    "pysparkDF = spark.createDataFrame(data = data, schema = columns)\n",
    "pysparkDF.printSchema()\n",
    "pysparkDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f58cbd-a684-478e-81f1-7769c7afb6c0",
   "metadata": {},
   "source": [
    "### Convert PySpark Dataframe to Pandas DataFrame\n",
    "PySpark DataFrame provides a method toPandas() to convert it to Python Pandas DataFrame. toPandas() results in the collection of all records in the PySpark DataFrame to the driver program and should be done only on a small subset of the data. Running on larger dataset’s results in memory error and crashes the application. To deal with a larger dataset, you can also try increasing memory on the driver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "abfe93a5-47bf-4f3c-aa3d-799e8d32d90c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  first_name middle_name last_name    dob gender  salary\n",
      "0      James                 Smith  36636      M   60000\n",
      "1    Michael        Rose            40288      M   70000\n",
      "2     Robert              Williams  42114         400000\n",
      "3      Maria        Anne     Jones  39192      F  500000\n",
      "4        Jen        Mary     Brown             F       0\n"
     ]
    }
   ],
   "source": [
    "pandasDF = pysparkDF.toPandas()\n",
    "print(pandasDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a8148e-8342-4d2f-b84f-0b06cfeb1623",
   "metadata": {},
   "source": [
    "Note that pandas add a sequence number to the result as a row Index. You can rename pandas columns by using rename() function.\n",
    "\n",
    "### Convert Spark Nested Struct DataFrame to Pandas \n",
    "Most of the time data in PySpark DataFrame will be in a structured format meaning one column contains other columns so let’s see how it convert to Pandas. Here is an example with nested struct where we have firstname, middlename and lastname are part of the name column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ec545223-b770-4dfe-bf87-158c69a7596c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: struct (nullable = true)\n",
      " |    |-- firstname: string (nullable = true)\n",
      " |    |-- middlename: string (nullable = true)\n",
      " |    |-- lastname: string (nullable = true)\n",
      " |-- dob: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: string (nullable = true)\n",
      "\n",
      "                   name    dob gender salary\n",
      "0      (James, , Smith)  36636      M   3000\n",
      "1     (Michael, Rose, )  40288      M   4000\n",
      "2  (Robert, , Williams)  42114      M   4000\n",
      "3  (Maria, Anne, Jones)  39192      F   4000\n",
      "4    (Jen, Mary, Brown)             F     -1\n"
     ]
    }
   ],
   "source": [
    "# Nested structure elements\n",
    "from pyspark.sql.types import StructType, StructField, StringType,IntegerType\n",
    "dataStruct = [((\"James\",\"\",\"Smith\"),\"36636\",\"M\",\"3000\"), \\\n",
    "      ((\"Michael\",\"Rose\",\"\"),\"40288\",\"M\",\"4000\"), \\\n",
    "      ((\"Robert\",\"\",\"Williams\"),\"42114\",\"M\",\"4000\"), \\\n",
    "      ((\"Maria\",\"Anne\",\"Jones\"),\"39192\",\"F\",\"4000\"), \\\n",
    "      ((\"Jen\",\"Mary\",\"Brown\"),\"\",\"F\",\"-1\") \\\n",
    "]\n",
    "\n",
    "schemaStruct = StructType([\n",
    "        StructField('name', StructType([\n",
    "             StructField('firstname', StringType(), True),\n",
    "             StructField('middlename', StringType(), True),\n",
    "             StructField('lastname', StringType(), True)\n",
    "             ])),\n",
    "          StructField('dob', StringType(), True),\n",
    "         StructField('gender', StringType(), True),\n",
    "         StructField('salary', StringType(), True)\n",
    "         ])\n",
    "\n",
    "df = spark.createDataFrame(data=dataStruct, schema = schemaStruct)\n",
    "df.printSchema()\n",
    "\n",
    "pandasDF2 = df.toPandas()\n",
    "print(pandasDF2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a513251-e034-470b-a8ec-1e1390125ef7",
   "metadata": {},
   "source": [
    "## PySpark show()\n",
    "PySpark DataFrame show() is used to display the contents of the DataFrame in a Table Row and Column Format. By default, it shows only 20 Rows, and the column values are truncated at 20 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "28e217e9-df9f-4766-9d95-6b75a6e7a46f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+------+------+\n",
      "|                name|  dob|gender|salary|\n",
      "+--------------------+-----+------+------+\n",
      "|    [James, , Smith]|36636|     M|  3000|\n",
      "|   [Michael, Rose, ]|40288|     M|  4000|\n",
      "|[Robert, , Williams]|42114|     M|  4000|\n",
      "|[Maria, Anne, Jones]|39192|     F|  4000|\n",
      "|  [Jen, Mary, Brown]|     |     F|    -1|\n",
      "+--------------------+-----+------+------+\n",
      "\n",
      "+--------------------+-----+------+------+\n",
      "|name                |dob  |gender|salary|\n",
      "+--------------------+-----+------+------+\n",
      "|[James, , Smith]    |36636|M     |3000  |\n",
      "|[Michael, Rose, ]   |40288|M     |4000  |\n",
      "|[Robert, , Williams]|42114|M     |4000  |\n",
      "|[Maria, Anne, Jones]|39192|F     |4000  |\n",
      "|[Jen, Mary, Brown]  |     |F     |-1    |\n",
      "+--------------------+-----+------+------+\n",
      "\n",
      "+-----------------+-----+------+------+\n",
      "|name             |dob  |gender|salary|\n",
      "+-----------------+-----+------+------+\n",
      "|[James, , Smith] |36636|M     |3000  |\n",
      "|[Michael, Rose, ]|40288|M     |4000  |\n",
      "+-----------------+-----+------+------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-----------------+-----+------+------+\n",
      "|             name|  dob|gender|salary|\n",
      "+-----------------+-----+------+------+\n",
      "| [James, , Smith]|36636|     M|  3000|\n",
      "|[Michael, Rose, ]|40288|     M|  4000|\n",
      "+-----------------+-----+------+------+\n",
      "only showing top 2 rows\n",
      "\n",
      "-RECORD 0----------------------\n",
      " name   | [James, , Smith]     \n",
      " dob    | 36636                \n",
      " gender | M                    \n",
      " salary | 3000                 \n",
      "-RECORD 1----------------------\n",
      " name   | [Michael, Rose, ]    \n",
      " dob    | 40288                \n",
      " gender | M                    \n",
      " salary | 4000                 \n",
      "-RECORD 2----------------------\n",
      " name   | [Robert, , Williams] \n",
      " dob    | 42114                \n",
      " gender | M                    \n",
      " salary | 4000                 \n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Default - displays 20 rows and \n",
    "# 20 charactes from column value \n",
    "df.show()\n",
    "\n",
    "#Display full column contents\n",
    "df.show(truncate=False)\n",
    "\n",
    "# Display 2 rows and full column contents\n",
    "df.show(2,truncate=False) \n",
    "\n",
    "# Display 2 rows & column values 25 characters\n",
    "df.show(2,truncate=25) \n",
    "\n",
    "# Display DataFrame rows & columns vertically\n",
    "df.show(n=3,truncate=25,vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041cad1a-d42e-45b2-9140-db51eace8868",
   "metadata": {},
   "source": [
    "### show() Syntax\n",
    "Following is the syntax of the show() function.\n",
    "def show(self, n=20, truncate=True, vertical=False):\n",
    "\n",
    "### PySpark show() To Display Contents\n",
    "Use PySpark show() method to display the contents of the DataFrame and use pyspark printSchema() method to print the schema. show() method by default shows only 20 rows/records from the DataFrame and truncates the column values at 20 characters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1e40ef88-ea96-4138-9683-4dc2a3e2bb3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|Seqno|               Quote|\n",
      "+-----+--------------------+\n",
      "|    1|Be the change tha...|\n",
      "|    2|Everyone thinks o...|\n",
      "|    3|The purpose of ou...|\n",
      "|    4|            Be cool.|\n",
      "+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns = [\"Seqno\",\"Quote\"]\n",
    "data = [(\"1\", \"Be the change that you wish to see in the world\"),\n",
    "    (\"2\", \"Everyone thinks of changing the world, but no one thinks of changing himself.\"),\n",
    "    (\"3\", \"The purpose of our lives is to be happy.\"),\n",
    "    (\"4\", \"Be cool.\")]\n",
    "df = spark.createDataFrame(data,columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5915831c-43b5-4e0c-91cf-50bb264c295e",
   "metadata": {},
   "source": [
    "values in the Quote column are truncated at 20 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b964f6d2-b35f-482d-adc4-e45b662695fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------------------------------------------------------------------+\n",
      "|Seqno|Quote                                                                        |\n",
      "+-----+-----------------------------------------------------------------------------+\n",
      "|1    |Be the change that you wish to see in the world                              |\n",
      "|2    |Everyone thinks of changing the world, but no one thinks of changing himself.|\n",
      "|3    |The purpose of our lives is to be happy.                                     |\n",
      "|4    |Be cool.                                                                     |\n",
      "+-----+-----------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Display full column contents\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "759f117d-0b04-41c2-8352-268d62c808c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------------------------------------------------------------------+\n",
      "|Seqno|Quote                                                                        |\n",
      "+-----+-----------------------------------------------------------------------------+\n",
      "|1    |Be the change that you wish to see in the world                              |\n",
      "|2    |Everyone thinks of changing the world, but no one thinks of changing himself.|\n",
      "+-----+-----------------------------------------------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display 2 rows and full column contents\n",
    "df.show(2,truncate=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4390db-4a25-4ef9-88db-fcaef0343020",
   "metadata": {},
   "source": [
    "### Show() with Truncate Column Values\n",
    "You can also truncate the column value at the desired length. By default it truncates after 20 characters however, you can display all contents by using truncate=False. If you wanted to truncate at a specific length use truncate=n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0909b9b3-036b-49d7-a167-2c405e3c0f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------------+\n",
      "|Seqno|                    Quote|\n",
      "+-----+-------------------------+\n",
      "|    1|Be the change that you...|\n",
      "|    2|Everyone thinks of cha...|\n",
      "+-----+-------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display 2 rows & column values 25 characters\n",
    "df.show(2,truncate=25) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200677d9-6493-4eaf-afa2-0588d998e4c1",
   "metadata": {},
   "source": [
    "### Display Contents Vertically\n",
    "Finally, let’s see how to display the DataFrame vertically record by record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0fd5e8b7-65db-4f36-977b-3c54299d2130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0--------------------------\n",
      " Seqno | 1                         \n",
      " Quote | Be the change that you... \n",
      "-RECORD 1--------------------------\n",
      " Seqno | 2                         \n",
      " Quote | Everyone thinks of cha... \n",
      "-RECORD 2--------------------------\n",
      " Seqno | 3                         \n",
      " Quote | The purpose of our liv... \n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display DataFrame rows & columns vertically\n",
    "df.show(n=3,truncate=25,vertical=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a5c5772-5c2e-42cb-b055-9f90e98532da",
   "metadata": {},
   "source": [
    "# PySpark SQL Functions\n",
    "## PySpark Aggregate Functions\n",
    "PySpark provides built-in standard Aggregate functions defines in DataFrame API, these come in handy when we need to make aggregate operations on DataFrame columns. Aggregate functions operate on a group of rows and calculate a single return value for every group.\n",
    "\n",
    "All these aggregate functions accept input as, Column type or column name in a string and several other arguments based on the function and return Column type.\n",
    "\n",
    "When possible try to leverage standard library as they are little bit more compile-time safety, handles null and perform better when compared to UDF’s. If your application is critical on performance try to avoid using custom UDF at all costs as these are not guarantee on performance.\n",
    "\n",
    "PySpark SQL Aggregate functions are grouped as “agg_funcs” in Pyspark. Below is a list of functions defined under this group.\n",
    "* approx_count_distinct\n",
    "* avg\n",
    "* collect_list\n",
    "* collect_set\n",
    "* countDistinct\n",
    "* count\n",
    "* grouping\n",
    "* first\n",
    "* last\n",
    "* kurtosis\n",
    "* max\n",
    "* min\n",
    "* mean\n",
    "* skewness\n",
    "* stddev\n",
    "* stddev_samp\n",
    "* stddev_pop\n",
    "* sum\n",
    "* sumDistinct\n",
    "* variance\n",
    "* var_samp\n",
    "* var_pop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15f7a5bf-9bf0-456e-bac9-47dfc624edcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"SparkByExamples.com\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a75a9ded-3920-473f-aaf2-5bb3ede0c8cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n",
      "+-------------+----------+------+\n",
      "|employee_name|department|salary|\n",
      "+-------------+----------+------+\n",
      "|James        |Sales     |3000  |\n",
      "|Michael      |Sales     |4600  |\n",
      "|Robert       |Sales     |4100  |\n",
      "|Maria        |Finance   |3000  |\n",
      "|James        |Sales     |3000  |\n",
      "|Scott        |Finance   |3300  |\n",
      "|Jen          |Finance   |3900  |\n",
      "|Jeff         |Marketing |3000  |\n",
      "|Kumar        |Marketing |2000  |\n",
      "|Saif         |Sales     |4100  |\n",
      "+-------------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "simpleData = [(\"James\", \"Sales\", 3000),\n",
    "    (\"Michael\", \"Sales\", 4600),\n",
    "    (\"Robert\", \"Sales\", 4100),\n",
    "    (\"Maria\", \"Finance\", 3000),\n",
    "    (\"James\", \"Sales\", 3000),\n",
    "    (\"Scott\", \"Finance\", 3300),\n",
    "    (\"Jen\", \"Finance\", 3900),\n",
    "    (\"Jeff\", \"Marketing\", 3000),\n",
    "    (\"Kumar\", \"Marketing\", 2000),\n",
    "    (\"Saif\", \"Sales\", 4100)\n",
    "  ]\n",
    "schema = [\"employee_name\", \"department\", \"salary\"]\n",
    "df = spark.createDataFrame(data=simpleData, schema = schema)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3df2fd-e1ec-4114-940b-c8da362aac36",
   "metadata": {},
   "source": [
    "### approx_count_distinct Aggregate Function\n",
    "In PySpark approx_count_distinct() function returns the count of distinct items in a group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f5016fc-975f-4e95-bf30-e264ed8b72a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "approx_count_distinct: 6\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import approx_count_distinct\n",
    "print(\"approx_count_distinct: \" + \\\n",
    "      str(df.select(approx_count_distinct(\"salary\")).collect()[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4600c1e2-513e-46b5-abe6-2bef0aa3e899",
   "metadata": {},
   "source": [
    "### avg (average) Aggregate Function\n",
    "avg() function returns the average of values in the input column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dea197a5-85d6-4877-b59d-7b44c9baaa95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg: 3400.0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "print(\"avg: \" + str(df.select(avg(\"salary\")).collect()[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df3ba59-503e-4b91-b0ff-a9c4ca8b88cd",
   "metadata": {},
   "source": [
    "### collect_list Aggregate Function\n",
    "collect_list() function returns all values from an input column with duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "223d1cb2-199f-4c21-a847-b95e19323810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------+\n",
      "|collect_list(salary)                                        |\n",
      "+------------------------------------------------------------+\n",
      "|[3000, 4600, 4100, 3000, 3000, 3300, 3900, 3000, 2000, 4100]|\n",
      "+------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import collect_list\n",
    "df.select(collect_list(\"salary\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d6f759-e4e4-474c-9198-47fa2ee3e6ec",
   "metadata": {},
   "source": [
    "### collect_set Aggregate Function\n",
    "collect_set() function returns all values from an input column with duplicate values eliminated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb2c47df-38a3-4080-87a6-263d91e61aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+\n",
      "|collect_set(salary)                 |\n",
      "+------------------------------------+\n",
      "|[4600, 3000, 3900, 4100, 3300, 2000]|\n",
      "+------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import collect_set\n",
    "df.select(collect_set(\"salary\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd51d4a-3cc6-47aa-a510-cf9ff858364a",
   "metadata": {},
   "source": [
    "### countDistinct Aggregate Function\n",
    "countDistinct() function returns the number of distinct elements in a columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76184c64-6ef2-475d-94db-e171268416c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+\n",
      "|count(DISTINCT department, salary)|\n",
      "+----------------------------------+\n",
      "|8                                 |\n",
      "+----------------------------------+\n",
      "\n",
      "Distinct Count of Department & Salary: 8\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import countDistinct\n",
    "df2 = df.select(countDistinct(\"department\", \"salary\"))\n",
    "df2.show(truncate=False)\n",
    "print(\"Distinct Count of Department & Salary: \"+str(df2.collect()[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841e4f0c-065f-48d4-abe5-bb6812553a35",
   "metadata": {},
   "source": [
    "### count function\n",
    "count() function returns number of elements in a column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f52cea38-e03d-49c1-aaad-be91598f73c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count: Row(count(salary)=10)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "print(\"count: \"+str(df.select(count(\"salary\")).collect()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e114da-04c1-4dde-b05d-38450b2f0013",
   "metadata": {},
   "source": [
    "### grouping function\n",
    "grouping() Indicates whether a given input column is aggregated or not. returns 1 for aggregated or 0 for not aggregated in the result. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694134c9-633f-4518-81ab-aa98d47a4844",
   "metadata": {},
   "source": [
    "### first function\n",
    "first() function returns the first element in a column when ignoreNulls is set to true, it returns the first non-null element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6dea60b9-2fbb-4c82-bed5-b41a713f9fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|first(salary)|\n",
      "+-------------+\n",
      "|3000         |\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import first\n",
    "df.select(first(\"salary\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf327f5c-98eb-4c22-bfc1-007011984ea8",
   "metadata": {},
   "source": [
    "### last function\n",
    "last() function returns the last element in a column. when ignoreNulls is set to true, it returns the last non-null element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e5201d4-0b79-455b-b60b-138c94e6c3fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|last(salary)|\n",
      "+------------+\n",
      "|4100        |\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import  last\n",
    "df.select(last(\"salary\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43cfae1-536f-4fbc-aaf4-c6ac10553301",
   "metadata": {},
   "source": [
    "### kurtosis function\n",
    "kurtosis() function returns the kurtosis of the values in a group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a236182d-3d39-4804-b619-68d7f17457c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|kurtosis(salary)   |\n",
      "+-------------------+\n",
      "|-0.6467803030303032|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import kurtosis\n",
    "df.select(kurtosis(\"salary\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948b9245-8b5a-4e7f-b9fd-db78770f52e6",
   "metadata": {},
   "source": [
    "### max function\n",
    "max() function returns the maximum value in a column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58b87b53-7c48-4182-8191-683dbbce3fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|max(salary)|\n",
      "+-----------+\n",
      "|4600       |\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import max\n",
    "df.select(max(\"salary\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5c4ab6-704a-4e67-96e1-9a454233c466",
   "metadata": {},
   "source": [
    "### min function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a09ed84a-b171-4231-ad77-d9c09838ab81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|min(salary)|\n",
      "+-----------+\n",
      "|2000       |\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import min\n",
    "df.select(min(\"salary\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970a7f9a-e6bc-433d-b410-f1df5df95af2",
   "metadata": {},
   "source": [
    "### mean function\n",
    "mean() function returns the average of the values in a column. Alias for Avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "85cbb40c-0983-4b5d-96e8-0761970a799d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|avg(salary)|\n",
      "+-----------+\n",
      "|3400.0     |\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import mean\n",
    "df.select(mean(\"salary\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bb8f34-9b31-4630-9f32-5de5165d0331",
   "metadata": {},
   "source": [
    "### skewness function\n",
    "skewness() function returns the skewness of the values in a group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "972ab05f-0660-4488-a085-079ae0f2f0e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|skewness(salary)    |\n",
      "+--------------------+\n",
      "|-0.12041791181069571|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import skewness\n",
    "df.select(skewness(\"salary\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707bccf3-817a-4ec3-96ea-c993746a7939",
   "metadata": {},
   "source": [
    "### stddev(), stddev_samp() and stddev_pop()\n",
    "stddev() alias for stddev_samp.\n",
    "\n",
    "stddev_samp() function returns the sample standard deviation of values in a column.\n",
    "\n",
    "stddev_pop() function returns the population standard deviation of the values in a column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f0a54d84-e974-43a7-add3-05df31a50b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+------------------+\n",
      "|stddev_samp(salary)|stddev_samp(salary)|stddev_pop(salary)|\n",
      "+-------------------+-------------------+------------------+\n",
      "|765.9416862050705  |765.9416862050705  |726.636084983398  |\n",
      "+-------------------+-------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import stddev, stddev_samp, stddev_pop\n",
    "df.select(stddev(\"salary\"), stddev_samp(\"salary\"), \\\n",
    "    stddev_pop(\"salary\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6fce49-1bb7-4417-b96c-9a618f1f8bdd",
   "metadata": {},
   "source": [
    "### sum function\n",
    "sum() function Returns the sum of all values in a column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "beebc065-5835-4bd4-8caa-dd876e659150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|sum(salary)|\n",
      "+-----------+\n",
      "|34000      |\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "df.select(sum(\"salary\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ef02ef-5f5e-4d84-9b12-4bf0444da11a",
   "metadata": {},
   "source": [
    "### sumDistinct function\n",
    "sumDistinct() function returns the sum of all distinct values in a column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "76efa0ca-febd-4330-ad04-556438b40fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|sum(DISTINCT salary)|\n",
      "+--------------------+\n",
      "|20900               |\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sumDistinct\n",
    "df.select(sumDistinct(\"salary\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ccdc06-71eb-44e0-b971-47d5bcfb52f4",
   "metadata": {},
   "source": [
    "### variance(), var_samp(), var_pop()\n",
    "variance() alias for var_samp\n",
    "\n",
    "var_samp() function returns the unbiased variance of the values in a column.\n",
    "\n",
    "var_pop() function returns the population variance of the values in a column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0ef1e253-6a49-496d-acf1-3bd2719911e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+---------------+\n",
      "|var_samp(salary) |var_samp(salary) |var_pop(salary)|\n",
      "+-----------------+-----------------+---------------+\n",
      "|586666.6666666666|586666.6666666666|528000.0       |\n",
      "+-----------------+-----------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import variance, var_samp, var_pop\n",
    "df.select(variance(\"salary\"),var_samp(\"salary\"),var_pop(\"salary\")) \\\n",
    "  .show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

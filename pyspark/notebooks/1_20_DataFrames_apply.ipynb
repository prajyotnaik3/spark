{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de59f7cf-f7da-4e01-9885-ebd10e84db20",
   "metadata": {},
   "source": [
    "# PySpark apply\n",
    "## PySpark apply Function to Column\n",
    "By using withColumn(), sql(), select() you can apply a built-in function or custom function to a column. In order to apply a custom function, first you need to create a function and register the function as a UDF. Recent versions of PySpark provide a way to use Pandas API hence, you can also use pyspark.pandas.DataFrame.apply()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39edaa12-dbcf-4194-b86f-62a73f582a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"SparkByExamples.com\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32805328-452b-4ef5-92e8-e704b5ff5c42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+\n",
      "|Seqno|Name        |\n",
      "+-----+------------+\n",
      "|1    |john jones  |\n",
      "|2    |tracey smith|\n",
      "|3    |amy sanders |\n",
      "+-----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns = [\"Seqno\",\"Name\"]\n",
    "data = [(\"1\", \"john jones\"),\n",
    "    (\"2\", \"tracey smith\"),\n",
    "    (\"3\", \"amy sanders\")]\n",
    "\n",
    "df = spark.createDataFrame(data=data,schema=columns)\n",
    "\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a043c2bf-c6fb-483c-8c13-dc41f72865ee",
   "metadata": {},
   "source": [
    "### PySpark apply Function using withColumn()\n",
    "PySpark withColumn() is a transformation function that is used to apply a function to the column. The below example applies an upper() function to column df.Name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf7c71e9-4672-4267-b9e8-36c412c43d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+------------+\n",
      "|Seqno|        Name|  Upper_Name|\n",
      "+-----+------------+------------+\n",
      "|    1|  john jones|  JOHN JONES|\n",
      "|    2|tracey smith|TRACEY SMITH|\n",
      "|    3| amy sanders| AMY SANDERS|\n",
      "+-----+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Apply function using withColumn\n",
    "from pyspark.sql.functions import upper\n",
    "df.withColumn(\"Upper_Name\", upper(df.Name)) \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b5f15f-5b47-4426-9e17-da0170f59d58",
   "metadata": {},
   "source": [
    "### Apply Function using select()\n",
    "The select() is used to select the columns from the PySpark DataFrame while selecting the columns you can also apply the function to a column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74fededa-d5b1-4987-a065-d06b45f8eb45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+------------+\n",
      "|Seqno|        Name| upper(Name)|\n",
      "+-----+------------+------------+\n",
      "|    1|  john jones|  JOHN JONES|\n",
      "|    2|tracey smith|TRACEY SMITH|\n",
      "|    3| amy sanders| AMY SANDERS|\n",
      "+-----+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Apply function using select  \n",
    "df.select(\"Seqno\",\"Name\", upper(df.Name)) \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077f6a50-e0a7-480d-9ea1-e0462dfec435",
   "metadata": {},
   "source": [
    "### Apply Function using sql()\n",
    "You can also apply the function to the column while running the SQL query on the PySpark DataFrame. In order to use SQL, make sure you create a temporary view using createOrReplaceTempView().<br>\n",
    "To run the SQL query use spark.sql() function and create the table by using createOrReplaceTempView(). This table would be available to use until you end your current SparkSession.<br>\n",
    "spark.sql() returns a DataFrame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5e85b93-9103-4077-9404-daae375d00d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+------------+\n",
      "|Seqno|        Name| upper(Name)|\n",
      "+-----+------------+------------+\n",
      "|    1|  john jones|  JOHN JONES|\n",
      "|    2|tracey smith|TRACEY SMITH|\n",
      "|    3| amy sanders| AMY SANDERS|\n",
      "+-----+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Apply function using sql()\n",
    "df.createOrReplaceTempView(\"TAB\")\n",
    "spark.sql(\"select Seqno, Name, UPPER(Name) from TAB\") \\\n",
    "     .show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eff8991-9b90-4a5c-ab06-f9e366ecfa55",
   "metadata": {},
   "source": [
    "### PySpark apply Custom UDF Function\n",
    "PySpark UDF (a.k.a User Defined Function) is the most useful feature of Spark SQL & DataFrame that is used to extend the PySpark built-in capabilities.<br>Note that UDFs are the most expensive operations hence use them only if you have no choice and when essential.<br>\n",
    "Following are the steps to apply a custom UDF function on an SQL query.\n",
    "#### Create Custom Function\n",
    "First, create a python function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e73b1e32-36c9-478d-bac0-602bc519f844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create custom function\n",
    "def upperCase(str):\n",
    "    return str.upper()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5394461-1243-4339-b131-1f50fb77df59",
   "metadata": {},
   "source": [
    "#### Register UDF\n",
    "Create a udf function by wrapping the above function with udf()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0652844e-320b-451e-aebe-097047d81471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert function to udf\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import StringType\n",
    "upperCaseUDF = udf(lambda x:upperCase(x),StringType()) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a059987d-60eb-4daa-8c1f-8edff77b35bc",
   "metadata": {},
   "source": [
    "#### Apply Custom UDF to Column\n",
    "Finally apply the function to the column by using withColumn(), select() and sql()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46ea27fe-9c50-4a24-8b61-c93384ab6532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+-------------+\n",
      "|Seqno|Name        |Cureated Name|\n",
      "+-----+------------+-------------+\n",
      "|1    |john jones  |JOHN JONES   |\n",
      "|2    |tracey smith|TRACEY SMITH |\n",
      "|3    |amy sanders |AMY SANDERS  |\n",
      "+-----+------------+-------------+\n",
      "\n",
      "+-----+------------+\n",
      "|Seqno|Name        |\n",
      "+-----+------------+\n",
      "|1    |JOHN JONES  |\n",
      "|2    |TRACEY SMITH|\n",
      "|3    |AMY SANDERS |\n",
      "+-----+------------+\n",
      "\n",
      "+-----+------------+------------------+\n",
      "|Seqno|        Name|upperCaseUDF(Name)|\n",
      "+-----+------------+------------------+\n",
      "|    1|  john jones|        JOHN JONES|\n",
      "|    2|tracey smith|      TRACEY SMITH|\n",
      "|    3| amy sanders|       AMY SANDERS|\n",
      "+-----+------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Custom UDF with withColumn()\n",
    "df.withColumn(\"Cureated Name\", upperCaseUDF(col(\"Name\"))) \\\n",
    "  .show(truncate=False)\n",
    "\n",
    "# Custom UDF with select()  \n",
    "df.select(col(\"Seqno\"), \\\n",
    "    upperCaseUDF(col(\"Name\")).alias(\"Name\") ) \\\n",
    "   .show(truncate=False)\n",
    "\n",
    "# Custom UDF with sql()\n",
    "spark.udf.register(\"upperCaseUDF\", upperCaseUDF)\n",
    "df.createOrReplaceTempView(\"TAB\")\n",
    "spark.sql(\"select Seqno, Name, upperCaseUDF(Name) from TAB\") \\\n",
    "     .show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4439cb-8bbb-4ad3-8ca1-8fd899e14c8d",
   "metadata": {},
   "source": [
    "### PySpark Pandas apply()\n",
    "PySpark DataFrame doesnâ€™t contain the apply() function. However, we can leverage Pandas DataFrame.apply() by running Pandas API over PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84a9ce2a-9370-4047-9a24-e2fd8815eeec",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark.pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13828\\3715979002.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Imports\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mps\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m technologies = ({\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyspark.pandas'"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import pyspark.pandas as ps\n",
    "import numpy as np\n",
    "\n",
    "technologies = ({\n",
    "    'Fee' :[20000,25000,30000,22000,np.NaN],\n",
    "    'Discount':[1000,2500,1500,1200,3000]\n",
    "               })\n",
    "# Create a DataFrame\n",
    "psdf = ps.DataFrame(technologies)\n",
    "print(psdf)\n",
    "\n",
    "def add(data):\n",
    "   return data[0] + data[1]\n",
    "   \n",
    "addDF = psdf.apply(add,axis=1)\n",
    "print(addDF)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

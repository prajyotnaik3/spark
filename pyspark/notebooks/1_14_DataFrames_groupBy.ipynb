{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97953e46-3fee-4008-a9bf-1fdd7ef2cc3f",
   "metadata": {},
   "source": [
    "# PySpark - groupBy()\n",
    "Similar to SQL GROUP BY clause, PySpark groupBy() function is used to collect the identical data into groups on DataFrame and perform count, sum, avg, min, max functions on the grouped data.\n",
    "\n",
    "### GroupBy() Syntax & Usage\n",
    "Syntax:<br>\n",
    "DataFrame.groupBy(*cols)<br>\n",
    "#or <br>\n",
    "DataFrame.groupby(*cols)<br>\n",
    "When we perform groupBy() on PySpark Dataframe, it returns GroupedData object which contains below aggregate functions.\n",
    "* count() – Use groupBy() count() to return the number of rows for each group.\n",
    "* mean() – Returns the mean of values for each group.\n",
    "* max() – Returns the maximum of values for each group.\n",
    "* min() – Returns the minimum of values for each group.\n",
    "* sum() – Returns the total for values for each group.\n",
    "* avg() – Returns the average for values for each group.\n",
    "* agg() – Using groupBy() agg() function, we can calculate more than one aggregate at a time.\n",
    "* pivot() – This function is used to Pivot the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "586e8d0a-16a8-429a-adb6-d4a1394fd8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"SparkByExamples.com\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb95f031-7af0-4cdb-b325-c983d7f5394b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- bonus: long (nullable = true)\n",
      "\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
      "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "|Raman        |Finance   |CA   |99000 |40 |24000|\n",
      "|Scott        |Finance   |NY   |83000 |36 |19000|\n",
      "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
      "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
      "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "simpleData = [(\"James\",\"Sales\",\"NY\",90000,34,10000),\n",
    "    (\"Michael\",\"Sales\",\"NY\",86000,56,20000),\n",
    "    (\"Robert\",\"Sales\",\"CA\",81000,30,23000),\n",
    "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000),\n",
    "    (\"Raman\",\"Finance\",\"CA\",99000,40,24000),\n",
    "    (\"Scott\",\"Finance\",\"NY\",83000,36,19000),\n",
    "    (\"Jen\",\"Finance\",\"NY\",79000,53,15000),\n",
    "    (\"Jeff\",\"Marketing\",\"CA\",80000,25,18000),\n",
    "    (\"Kumar\",\"Marketing\",\"NY\",91000,50,21000)\n",
    "  ]\n",
    "\n",
    "schema = [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\n",
    "df = spark.createDataFrame(data=simpleData, schema = schema)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b423c6-2a27-4e02-bc33-02e65045e71e",
   "metadata": {},
   "source": [
    "## PySpark groupBy on DataFrame Columns\n",
    "Let’s do the groupBy() on department column of DataFrame and then find the sum of salary for each department using sum() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9d831e4-5cdc-497b-a6ea-22bb85fe345a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|department|sum(salary)|\n",
      "+----------+-----------+\n",
      "|Sales     |257000     |\n",
      "|Finance   |351000     |\n",
      "|Marketing |171000     |\n",
      "+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"department\").sum(\"salary\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1901789-9a3f-4d9b-8f69-31b8ff24af3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|department|count|\n",
      "+----------+-----+\n",
      "|     Sales|    3|\n",
      "|   Finance|    4|\n",
      "| Marketing|    2|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"department\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2d34b0d-b02a-4118-9905-6b8aad4cf083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|department|min(salary)|\n",
      "+----------+-----------+\n",
      "|     Sales|      81000|\n",
      "|   Finance|      79000|\n",
      "| Marketing|      80000|\n",
      "+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"department\").min(\"salary\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93640c90-2078-4735-973c-a001acd424a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|department|max(salary)|\n",
      "+----------+-----------+\n",
      "|     Sales|      90000|\n",
      "|   Finance|      99000|\n",
      "| Marketing|      91000|\n",
      "+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"department\").max(\"salary\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1b538d9-ca56-48b4-a711-e84a7184f501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+\n",
      "|department|      avg(salary)|\n",
      "+----------+-----------------+\n",
      "|     Sales|85666.66666666667|\n",
      "|   Finance|          87750.0|\n",
      "| Marketing|          85500.0|\n",
      "+----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"department\").avg( \"salary\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef6fc2d3-59aa-4cf1-8e9d-82a252bf1255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+\n",
      "|department|      avg(salary)|\n",
      "+----------+-----------------+\n",
      "|     Sales|85666.66666666667|\n",
      "|   Finance|          87750.0|\n",
      "| Marketing|          85500.0|\n",
      "+----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"department\").mean( \"salary\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056837a6-20d1-44d4-8643-ef386cf8eed2",
   "metadata": {},
   "source": [
    "### Using Multiple columns\n",
    "Similarly, we can also run groupBy and aggregate on two or more DataFrame columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fbb5e4da-9613-4c7c-9ce9-881bfa032ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-----------+----------+\n",
      "|department|state|sum(salary)|sum(bonus)|\n",
      "+----------+-----+-----------+----------+\n",
      "|Finance   |NY   |162000     |34000     |\n",
      "|Marketing |NY   |91000      |21000     |\n",
      "|Sales     |CA   |81000      |23000     |\n",
      "|Marketing |CA   |80000      |18000     |\n",
      "|Finance   |CA   |189000     |47000     |\n",
      "|Sales     |NY   |176000     |30000     |\n",
      "+----------+-----+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#GroupBy on multiple columns\n",
    "df.groupBy(\"department\",\"state\") \\\n",
    "    .sum(\"salary\",\"bonus\") \\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11a8973-6064-4c7e-9ab9-980db68d7605",
   "metadata": {},
   "source": [
    "### Running more aggregates at a time\n",
    "Using agg() aggregate function we can calculate many aggregations at a time on a single statement using SQL functions sum(), avg(), min(), max() mean() e.t.c. In order to use these, we should import \"from pyspark.sql.functions import sum,avg,max,min,mean,count\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cbeebc45-459b-4c4a-b77e-05ec6e01fae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----------------+---------+---------+\n",
      "|department|sum_salary|avg_salary       |sum_bonus|max_bonus|\n",
      "+----------+----------+-----------------+---------+---------+\n",
      "|Sales     |257000    |85666.66666666667|53000    |23000    |\n",
      "|Finance   |351000    |87750.0          |81000    |24000    |\n",
      "|Marketing |171000    |85500.0          |39000    |21000    |\n",
      "+----------+----------+-----------------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum,avg,max\n",
    "df.groupBy(\"department\") \\\n",
    "    .agg(sum(\"salary\").alias(\"sum_salary\"), \\\n",
    "         avg(\"salary\").alias(\"avg_salary\"), \\\n",
    "         sum(\"bonus\").alias(\"sum_bonus\"), \\\n",
    "         max(\"bonus\").alias(\"max_bonus\") \\\n",
    "     ) \\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5ac50c-0e95-4815-904a-af49c7f55d97",
   "metadata": {},
   "source": [
    "### Using filter on aggregate data\n",
    "Similar to SQL “HAVING” clause, On PySpark DataFrame we can use either where() or filter() function to filter the rows of aggregated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f33af5e-70eb-4c6e-82dc-a6190ac80c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----------------+---------+---------+\n",
      "|department|sum_salary|avg_salary       |sum_bonus|max_bonus|\n",
      "+----------+----------+-----------------+---------+---------+\n",
      "|Sales     |257000    |85666.66666666667|53000    |23000    |\n",
      "|Finance   |351000    |87750.0          |81000    |24000    |\n",
      "+----------+----------+-----------------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col,sum,avg,max\n",
    "df.groupBy(\"department\") \\\n",
    "    .agg(sum(\"salary\").alias(\"sum_salary\"), \\\n",
    "      avg(\"salary\").alias(\"avg_salary\"), \\\n",
    "      sum(\"bonus\").alias(\"sum_bonus\"), \\\n",
    "      max(\"bonus\").alias(\"max_bonus\")) \\\n",
    "    .where(col(\"sum_bonus\") >= 50000) \\\n",
    "    .show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e5ee06c-d955-4316-8e47-fcb3eadc3b1d",
   "metadata": {},
   "source": [
    "## PySpark Window Functions\n",
    "PySpark Window functions are used to calculate results such as the rank, row number e.t.c over a range of input rows. These come in handy when we need to make aggregate operations in a specific window frame on DataFrame columns.\n",
    "\n",
    "When possible try to leverage standard library as they are little bit more compile-time safety, handles null and perform better when compared to UDFâ€™s. If your application is critical on performance try to avoid using custom UDF at all costs as these are not guarantee on performance.\n",
    "\n",
    "PySpark Window functions operate on a group of rows (like frame, partition) and return a single value for every input row. PySpark SQL supports three kinds of window functions:\n",
    "* ranking functions\n",
    "* analytic functions\n",
    "* aggregate functions\n",
    "\n",
    "To perform an operation on a group first, we need to partition the data using Window.partitionBy() , and for row number and rank function we need to additionally order by on partition data using orderBy clause."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f56617ab-84e4-429c-9407-02d2a41a8723",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"SparkByExamples.com\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c2f4da3-c03e-4cf7-922b-82d753319cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n",
      "+-------------+----------+------+\n",
      "|employee_name|department|salary|\n",
      "+-------------+----------+------+\n",
      "|James        |Sales     |3000  |\n",
      "|Michael      |Sales     |4600  |\n",
      "|Robert       |Sales     |4100  |\n",
      "|Maria        |Finance   |3000  |\n",
      "|James        |Sales     |3000  |\n",
      "|Scott        |Finance   |3300  |\n",
      "|Jen          |Finance   |3900  |\n",
      "|Jeff         |Marketing |3000  |\n",
      "|Kumar        |Marketing |2000  |\n",
      "|Saif         |Sales     |4100  |\n",
      "+-------------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "simpleData = ((\"James\", \"Sales\", 3000), \\\n",
    "    (\"Michael\", \"Sales\", 4600),  \\\n",
    "    (\"Robert\", \"Sales\", 4100),   \\\n",
    "    (\"Maria\", \"Finance\", 3000),  \\\n",
    "    (\"James\", \"Sales\", 3000),    \\\n",
    "    (\"Scott\", \"Finance\", 3300),  \\\n",
    "    (\"Jen\", \"Finance\", 3900),    \\\n",
    "    (\"Jeff\", \"Marketing\", 3000), \\\n",
    "    (\"Kumar\", \"Marketing\", 2000),\\\n",
    "    (\"Saif\", \"Sales\", 4100) \\\n",
    "  )\n",
    " \n",
    "columns= [\"employee_name\", \"department\", \"salary\"]\n",
    "df = spark.createDataFrame(data = simpleData, schema = columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b739dd-c40d-4a23-afeb-a6320a7d948b",
   "metadata": {},
   "source": [
    "### PySpark Window Ranking functions\n",
    "#### row_number Window Function\n",
    "row_number() window function is used to give the sequential row number starting from 1 to the result of each window partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edb7c558-2b23-4232-8946-dcc9ae024532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----------+\n",
      "|employee_name|department|salary|row_number|\n",
      "+-------------+----------+------+----------+\n",
      "|James        |Sales     |3000  |1         |\n",
      "|James        |Sales     |3000  |2         |\n",
      "|Robert       |Sales     |4100  |3         |\n",
      "|Saif         |Sales     |4100  |4         |\n",
      "|Michael      |Sales     |4600  |5         |\n",
      "|Maria        |Finance   |3000  |1         |\n",
      "|Scott        |Finance   |3300  |2         |\n",
      "|Jen          |Finance   |3900  |3         |\n",
      "|Kumar        |Marketing |2000  |1         |\n",
      "|Jeff         |Marketing |3000  |2         |\n",
      "+-------------+----------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "windowSpec  = Window.partitionBy(\"department\").orderBy(\"salary\")\n",
    "\n",
    "df.withColumn(\"row_number\",row_number().over(windowSpec)) \\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d12e58-129e-4df1-9432-792521a79d35",
   "metadata": {},
   "source": [
    "#### rank Window Function\n",
    "rank() window function is used to provide a rank to the result within a window partition. This function leaves gaps in rank when there are ties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4df59868-1972-4bc2-9d4b-2269b22f0094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----+\n",
      "|employee_name|department|salary|rank|\n",
      "+-------------+----------+------+----+\n",
      "|        James|     Sales|  3000|   1|\n",
      "|        James|     Sales|  3000|   1|\n",
      "|       Robert|     Sales|  4100|   3|\n",
      "|         Saif|     Sales|  4100|   3|\n",
      "|      Michael|     Sales|  4600|   5|\n",
      "|        Maria|   Finance|  3000|   1|\n",
      "|        Scott|   Finance|  3300|   2|\n",
      "|          Jen|   Finance|  3900|   3|\n",
      "|        Kumar| Marketing|  2000|   1|\n",
      "|         Jeff| Marketing|  3000|   2|\n",
      "+-------------+----------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"rank\"\"\"\n",
    "from pyspark.sql.functions import rank\n",
    "df.withColumn(\"rank\",rank().over(windowSpec)) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423f97e6-abe1-4801-bc7a-86e173e9eb17",
   "metadata": {},
   "source": [
    "#### dense_rank Window Function\n",
    "dense_rank() window function is used to get the result with rank of rows within a window partition without any gaps. This is similar to rank() function difference being rank function leaves gaps in rank when there are ties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8152cf1a-faf4-460e-a9cd-429f16bd7774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----------+\n",
      "|employee_name|department|salary|dense_rank|\n",
      "+-------------+----------+------+----------+\n",
      "|        James|     Sales|  3000|         1|\n",
      "|        James|     Sales|  3000|         1|\n",
      "|       Robert|     Sales|  4100|         2|\n",
      "|         Saif|     Sales|  4100|         2|\n",
      "|      Michael|     Sales|  4600|         3|\n",
      "|        Maria|   Finance|  3000|         1|\n",
      "|        Scott|   Finance|  3300|         2|\n",
      "|          Jen|   Finance|  3900|         3|\n",
      "|        Kumar| Marketing|  2000|         1|\n",
      "|         Jeff| Marketing|  3000|         2|\n",
      "+-------------+----------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"dense_rank\"\"\"\n",
    "from pyspark.sql.functions import dense_rank\n",
    "df.withColumn(\"dense_rank\",dense_rank().over(windowSpec)) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d1e1c8-353a-44f8-af19-01027d803463",
   "metadata": {},
   "source": [
    "#### percent_rank Window Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a0294de-9d3d-44e4-85d4-879dfe9168f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+------------+\n",
      "|employee_name|department|salary|percent_rank|\n",
      "+-------------+----------+------+------------+\n",
      "|        James|     Sales|  3000|         0.0|\n",
      "|        James|     Sales|  3000|         0.0|\n",
      "|       Robert|     Sales|  4100|         0.5|\n",
      "|         Saif|     Sales|  4100|         0.5|\n",
      "|      Michael|     Sales|  4600|         1.0|\n",
      "|        Maria|   Finance|  3000|         0.0|\n",
      "|        Scott|   Finance|  3300|         0.5|\n",
      "|          Jen|   Finance|  3900|         1.0|\n",
      "|        Kumar| Marketing|  2000|         0.0|\n",
      "|         Jeff| Marketing|  3000|         1.0|\n",
      "+-------------+----------+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" percent_rank \"\"\"\n",
    "from pyspark.sql.functions import percent_rank\n",
    "df.withColumn(\"percent_rank\",percent_rank().over(windowSpec)) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0065097a-d440-46fa-9ce4-1192050a48b6",
   "metadata": {},
   "source": [
    "#### ntile Window Function\n",
    "ntile() window function returns the relative rank of result rows within a window partition. In below example we have used 2 as an argument to ntile hence it returns ranking between 2 values (1 and 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c731504d-c2c7-40c6-849f-ef437724c08c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+-----+\n",
      "|employee_name|department|salary|ntile|\n",
      "+-------------+----------+------+-----+\n",
      "|        James|     Sales|  3000|    1|\n",
      "|        James|     Sales|  3000|    1|\n",
      "|       Robert|     Sales|  4100|    1|\n",
      "|         Saif|     Sales|  4100|    2|\n",
      "|      Michael|     Sales|  4600|    2|\n",
      "|        Maria|   Finance|  3000|    1|\n",
      "|        Scott|   Finance|  3300|    1|\n",
      "|          Jen|   Finance|  3900|    2|\n",
      "|        Kumar| Marketing|  2000|    1|\n",
      "|         Jeff| Marketing|  3000|    2|\n",
      "+-------------+----------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"ntile\"\"\"\n",
    "from pyspark.sql.functions import ntile\n",
    "df.withColumn(\"ntile\",ntile(2).over(windowSpec)) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cf999b-f97b-463a-879d-5b093378b9c3",
   "metadata": {},
   "source": [
    "### PySpark Window Analytic functions\n",
    "#### cume_dist Window Function\n",
    "cume_dist() window function is used to get the cumulative distribution of values within a window partition.\n",
    "\n",
    "This is the same as the DENSE_RANK function in SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ede96ae2-6237-4d3f-a9df-95c330476a6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+------------------+\n",
      "|employee_name|department|salary|         cume_dist|\n",
      "+-------------+----------+------+------------------+\n",
      "|        James|     Sales|  3000|               0.4|\n",
      "|        James|     Sales|  3000|               0.4|\n",
      "|       Robert|     Sales|  4100|               0.8|\n",
      "|         Saif|     Sales|  4100|               0.8|\n",
      "|      Michael|     Sales|  4600|               1.0|\n",
      "|        Maria|   Finance|  3000|0.3333333333333333|\n",
      "|        Scott|   Finance|  3300|0.6666666666666666|\n",
      "|          Jen|   Finance|  3900|               1.0|\n",
      "|        Kumar| Marketing|  2000|               0.5|\n",
      "|         Jeff| Marketing|  3000|               1.0|\n",
      "+-------------+----------+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" cume_dist \"\"\"\n",
    "from pyspark.sql.functions import cume_dist    \n",
    "df.withColumn(\"cume_dist\",cume_dist().over(windowSpec)) \\\n",
    "   .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e79f914-770c-4558-b867-f838321e35f6",
   "metadata": {},
   "source": [
    "#### lag Window Function\n",
    "This is the same as the LAG function in SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41c3c8e1-11d9-4a1c-af34-411c8a85fbea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----+\n",
      "|employee_name|department|salary| lag|\n",
      "+-------------+----------+------+----+\n",
      "|        James|     Sales|  3000|null|\n",
      "|        James|     Sales|  3000|null|\n",
      "|       Robert|     Sales|  4100|3000|\n",
      "|         Saif|     Sales|  4100|3000|\n",
      "|      Michael|     Sales|  4600|4100|\n",
      "|        Maria|   Finance|  3000|null|\n",
      "|        Scott|   Finance|  3300|null|\n",
      "|          Jen|   Finance|  3900|3000|\n",
      "|        Kumar| Marketing|  2000|null|\n",
      "|         Jeff| Marketing|  3000|null|\n",
      "+-------------+----------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"lag\"\"\"\n",
    "from pyspark.sql.functions import lag    \n",
    "df.withColumn(\"lag\",lag(\"salary\",2).over(windowSpec)) \\\n",
    "      .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36e6600-1738-4bf5-8b6d-f52dbcc88862",
   "metadata": {},
   "source": [
    "#### lead Window Function\n",
    "This is the same as the LEAD function in SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c042339-f250-4696-8a7a-fa59f6bca54c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----+\n",
      "|employee_name|department|salary|lead|\n",
      "+-------------+----------+------+----+\n",
      "|        James|     Sales|  3000|4100|\n",
      "|        James|     Sales|  3000|4100|\n",
      "|       Robert|     Sales|  4100|4600|\n",
      "|         Saif|     Sales|  4100|null|\n",
      "|      Michael|     Sales|  4600|null|\n",
      "|        Maria|   Finance|  3000|3900|\n",
      "|        Scott|   Finance|  3300|null|\n",
      "|          Jen|   Finance|  3900|null|\n",
      "|        Kumar| Marketing|  2000|null|\n",
      "|         Jeff| Marketing|  3000|null|\n",
      "+-------------+----------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    " \"\"\"lead\"\"\"\n",
    "from pyspark.sql.functions import lead    \n",
    "df.withColumn(\"lead\",lead(\"salary\",2).over(windowSpec)) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29975d4e-2cdc-4b1f-9158-8e0b610d08f8",
   "metadata": {},
   "source": [
    "### PySpark Window Aggregate Functions\n",
    "When working with Aggregate functions, we donâ€™t need to use order by clause."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a88cd1c-4b33-46af-b807-0da5a15f2959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+-----+----+----+\n",
      "|department|   avg|  sum| min| max|\n",
      "+----------+------+-----+----+----+\n",
      "|     Sales|3760.0|18800|3000|4600|\n",
      "|   Finance|3400.0|10200|3000|3900|\n",
      "| Marketing|2500.0| 5000|2000|3000|\n",
      "+----------+------+-----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "windowSpecAgg  = Window.partitionBy(\"department\")\n",
    "from pyspark.sql.functions import col,avg,sum,min,max,row_number \n",
    "df.withColumn(\"row\",row_number().over(windowSpec)) \\\n",
    "  .withColumn(\"avg\", avg(col(\"salary\")).over(windowSpecAgg)) \\\n",
    "  .withColumn(\"sum\", sum(col(\"salary\")).over(windowSpecAgg)) \\\n",
    "  .withColumn(\"min\", min(col(\"salary\")).over(windowSpecAgg)) \\\n",
    "  .withColumn(\"max\", max(col(\"salary\")).over(windowSpecAgg)) \\\n",
    "  .where(col(\"row\")==1).select(\"department\",\"avg\",\"sum\",\"min\",\"max\") \\\n",
    "  .show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

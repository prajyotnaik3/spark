{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2b88268-1344-4c43-beef-f7337aebb610",
   "metadata": {},
   "source": [
    "# PySpark parallelize\n",
    "## PySpark parallelize() – Create RDD from a list data\n",
    "PySpark parallelize() is a function in SparkContext and is used to create an RDD from a list collection. \n",
    "\n",
    "Resilient Distributed Datasets (RDD) is a fundamental data structure of PySpark, It is an immutable distributed collection of objects. Each dataset in RDD is divided into logical partitions, which may be computed on different nodes of the cluster.\n",
    "* PySpark Parallelizing an existing collection in your driver program.\n",
    "\n",
    "Below is an example of how to create an RDD using a parallelize method from Sparkcontext. sparkContext.parallelize([1,2,3,4,5,6,7,8,9,10]) creates an RDD with a list of Integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d00f6423-f476-4053-a664-3b782e2a3647",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef151a3-86e8-4772-b58e-e6a7207de7aa",
   "metadata": {},
   "source": [
    "### Using sc.parallelize on PySpark Shell or REPL\n",
    "PySpark shell provides SparkContext variable “sc”, use sc.parallelize() to create an RDD.\n",
    "\n",
    "rdd = sc.parallelize([1,2,3,4,5,6,7,8,9,10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7800bf-a975-49b5-b7cd-693979fa2255",
   "metadata": {},
   "source": [
    "#### Using PySpark sparkContext.parallelize() in application\n",
    "Since PySpark 2.0, First, you need to create a SparkSession which internally creates a SparkContext for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a88091e2-b8ad-437c-8472-caad02d9dda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "sparkContext=spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd6f5c0-d96c-4bae-a48d-9f0dbaa329b1",
   "metadata": {},
   "source": [
    "Now, use sparkContext.parallelize() to create rdd from a list or collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d881aa3-5bba-4cdb-bfd2-8c83c6ed0ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Partitions: 1\n",
      "Action: First element: 1\n",
      "[1, 2, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "rdd=sparkContext.parallelize([1,2,3,4,5])\n",
    "rddCollect = rdd.collect()\n",
    "print(\"Number of Partitions: \"+str(rdd.getNumPartitions()))\n",
    "print(\"Action: First element: \"+str(rdd.first()))\n",
    "print(rddCollect)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28fb2d5-dcf1-4ced-9324-32f04ee14d5c",
   "metadata": {},
   "source": [
    "parallelize() function also has another signature which additionally takes integer argument to specifies the number of partitions. Partitions are basic units of parallelism in PySpark.\n",
    "\n",
    "Remember, RDDs in PySpark are a collection of partitions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec67be7-1cdc-4183-b741-c5731456f04a",
   "metadata": {},
   "source": [
    "#### create empty RDD by using sparkContext.parallelize\n",
    "Some times we may need to create empty RDD and you can also use parallelize() in order to create it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91a8fed4-e7ba-4d13-a0b6-ece48ce42286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is Empty RDD : True\n"
     ]
    }
   ],
   "source": [
    "emptyRDD = sparkContext.emptyRDD()\n",
    "emptyRDD2 = rdd=sparkContext.parallelize([])\n",
    "\n",
    "print(\"is Empty RDD : \"+str(emptyRDD2.isEmpty()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a23868e-adaf-4f27-82b5-8f526ce5cea3",
   "metadata": {},
   "source": [
    "# PySpark - repartition() and coalesce()\n",
    "repartition() is used to increase or decrease the RDD/DataFrame partitions whereas the PySpark coalesce() is used to only decrease the number of partitions in an efficient way.\n",
    "\n",
    "Note: PySpark repartition() and coalesce() are very expensive operations as they shuffle the data across many partitions hence try to minimize using these as much as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a963b9a9-4b4f-429e-bd24-e9934413e10e",
   "metadata": {},
   "source": [
    "In RDD, you can create parallelism at the time of the creation of an RDD using parallelize(), textFile() and wholeTextFiles()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c8c9837-c589-4c7c-ac4c-da2264c234a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "# Create SparkSession from builder\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[1]\") \\\n",
    "                    .appName('SparkByExamples.com') \\\n",
    "                    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af34ab64-e940-47d8-9013-20ec50bb0701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From local[5] : 1\n",
      "parallelize : 6\n",
      "TextFile : 10\n"
     ]
    }
   ],
   "source": [
    "# Create spark session with local[5]\n",
    "rdd = spark.sparkContext.parallelize(range(0,20))\n",
    "print(\"From local[5] : \"+str(rdd.getNumPartitions()))\n",
    "\n",
    "# Use parallelize with 6 partitions\n",
    "rdd1 = spark.sparkContext.parallelize(range(0,25), 6)\n",
    "print(\"parallelize : \"+str(rdd1.getNumPartitions()))\n",
    "\n",
    "rddFromFile = spark.sparkContext.textFile(\"../resources/tmp/test.txt\",10)\n",
    "print(\"TextFile : \"+str(rddFromFile.getNumPartitions()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd1b012-f911-47fd-a22b-28c323b03dbf",
   "metadata": {},
   "source": [
    "## RDD repartition()\n",
    "repartition() method is used to increase or decrease the partitions. The below example decreases the partitions from 10 to 4 by moving data from all partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6519ab7-c072-4ff4-93a1-5afb692b9835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repartition size : 4\n"
     ]
    }
   ],
   "source": [
    "# Using repartition\n",
    "rdd2 = rdd1.repartition(4)\n",
    "print(\"Repartition size : \"+str(rdd2.getNumPartitions()))\n",
    "rdd2.saveAsTextFile(\"../resources/tmp/re-partition\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d9a0d9-f641-407a-b6de-be58804ff785",
   "metadata": {},
   "source": [
    "## coalesce()\n",
    "RDD coalesce() is used only to reduce the number of partitions. This is an optimized or improved version of repartition() where the movement of the data across the partitions is lower using coalesce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06df09f9-adaf-464b-b0a6-3db7c9aea7db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repartition size : 4\n"
     ]
    }
   ],
   "source": [
    "# Using coalesce()\n",
    "rdd3 = rdd1.coalesce(4)\n",
    "print(\"Repartition size : \"+str(rdd3.getNumPartitions()))\n",
    "rdd3.saveAsTextFile(\"/tmp/coalesce\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70d6d3c-659b-411a-a9d7-d79bf392870e",
   "metadata": {},
   "source": [
    "## PySpark DataFrame repartition() vs coalesce()\n",
    "Like RDD, you canâ€™t specify the partition/parallelism while creating DataFrame. DataFrame by default internally uses the methods specified in Section 1 to determine the default partition and splits the data for parallelism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a37c62c-8411-4aa9-8f61-fc1e3115af7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# DataFrame example\n",
    "df=spark.range(0,20)\n",
    "print(df.rdd.getNumPartitions())\n",
    "\n",
    "df.write.mode(\"overwrite\").csv(\"../resources/tmp/partition.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37bfd20-5981-4e2e-a1bd-c5517e92361e",
   "metadata": {},
   "source": [
    "### DataFrame repartition()\n",
    "Similar to RDD, the PySpark DataFrame repartition() method is used to increase or decrease the partitions. The below example increases the partitions from 5 to 6 by moving data from all partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91e1722c-7874-4370-984b-135c65527d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "# DataFrame repartition\n",
    "df2 = df.repartition(6)\n",
    "print(df2.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1bb288-b585-4a60-b936-422dd7d553dc",
   "metadata": {},
   "source": [
    "And, even decreasing the partitions also results in moving data from all partitions. hence when you wanted to decrease the partition recommendation is to use coalesce()/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76c2208-0869-41b6-8132-4a3616438840",
   "metadata": {},
   "source": [
    "### DataFrame coalesce()\n",
    "Spark DataFrame coalesce() is used only to decrease the number of partitions. This is an optimized or improved version of repartition() where the movement of the data across the partitions is fewer using coalesce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e2648ae-a575-4ec4-9a65-98e50ef65afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# DataFrame coalesce\n",
    "df3 = df.coalesce(2)\n",
    "print(df3.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890532ec-cc14-43c4-bccd-9911934e5949",
   "metadata": {},
   "source": [
    "Default Shuffle Partition\n",
    "Calling groupBy(), union(), join() and similar functions on DataFrame results in shuffling data between multiple executors and even machines and finally repartitions data into 200 partitions by default. PySpark default defines shuffling partition to 200 using spark.sql.shuffle.partitions configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0eea8598-26fa-4adb-9ded-7c4fddac2253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# Default shuffle partition count\n",
    "df4 = df.groupBy(\"id\").count()\n",
    "print(df4.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c109b3-e722-45df-a9a7-54cf7b43907f",
   "metadata": {},
   "source": [
    "Post shuffle operations, you can change the partitions either using coalesce() or repartition()."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

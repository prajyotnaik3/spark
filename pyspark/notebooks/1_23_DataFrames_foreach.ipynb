{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "502ea85e-c3b6-4648-a4f9-f94ce1928dee",
   "metadata": {},
   "source": [
    "## PySpark foreach()\n",
    "PySpark foreach() is an action operation that is available in RDD, DataFram to iterate/loop over each element in the DataFrmae, It is similar to for with advanced concepts. This is different than other actions as foreach() function doesn’t return a value instead it executes the input function on each element of an RDD, DataFrame\n",
    "### PySpark DataFrame foreach()\n",
    "Following is the syntax of the foreach() function<br>\n",
    "DataFrame.foreach(f)<br>\n",
    "When foreach() applied on PySpark DataFrame, it executes a function specified in for each element of DataFrame. This operation is mainly used if you wanted to manipulate accumulators, save the DataFrame results to RDBMS tables, Kafka topics, and other external sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1056822-b7d6-4c83-bb11-b6247de8fb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"SparkByExamples.com\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bbdac4f-6c13-4bdc-bebe-571655d369f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+\n",
      "|Seqno|        Name|\n",
      "+-----+------------+\n",
      "|    1|  john jones|\n",
      "|    2|tracey smith|\n",
      "|    3| amy sanders|\n",
      "+-----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prepare Data\n",
    "columns = [\"Seqno\",\"Name\"]\n",
    "data = [(\"1\", \"john jones\"),\n",
    "    (\"2\", \"tracey smith\"),\n",
    "    (\"3\", \"amy sanders\")]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data=data,schema=columns)\n",
    "df.show()\n",
    "\n",
    "# foreach() Example\n",
    "def f(df):\n",
    "    print(df.Seqno)\n",
    "df.foreach(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "809c7ef7-8727-4e19-a5da-c771295b8676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "# foreach() with accumulator Example\n",
    "accum=spark.sparkContext.accumulator(0)\n",
    "df.foreach(lambda x:accum.add(int(x.Seqno)))\n",
    "print(accum.value) #Accessed by driver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6fde4e-b770-47ab-b0b9-aa1656506f4a",
   "metadata": {},
   "source": [
    "### PySpark RDD foreach() Usage\n",
    "The foreach() on RDD behaves similarly to DataFrame equivalent, hence the same syntax and it is also used to manipulate accumulators from RDD, and write external data sources.\n",
    "\n",
    "RDD.foreach(f: Callable[[T], None]) → None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "903d0218-0b3b-4980-9864-396128cd2da7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "# foreach() with RDD example\n",
    "accum=spark.sparkContext.accumulator(0)\n",
    "rdd=spark.sparkContext.parallelize([1,2,3,4,5])\n",
    "rdd.foreach(lambda x:accum.add(x))\n",
    "print(accum.value) #Accessed by driver"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
